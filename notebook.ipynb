{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Minimal embeddings example\n",
       "\n",
       "## with TensorFlow and the iris dataset\n",
       "\n",
       "#### by August Karlstedt\n",
       "\n",
       "todo: write\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, Video\n",
    "with open('README.md') as f:\n",
    "    display(Markdown('\\n'.join(f.readlines())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set TF random seed to improve reproducibility\n",
    "tf.set_random_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sepal length</th>\n",
       "      <td>150.0</td>\n",
       "      <td>5.843333</td>\n",
       "      <td>0.828066</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.80</td>\n",
       "      <td>6.4</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sepal width</th>\n",
       "      <td>150.0</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petal length</th>\n",
       "      <td>150.0</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>4.35</td>\n",
       "      <td>5.1</td>\n",
       "      <td>6.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petal width</th>\n",
       "      <td>150.0</td>\n",
       "      <td>1.199333</td>\n",
       "      <td>0.762238</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count      mean       std  min  25%   50%  75%  max\n",
       "sepal length  150.0  5.843333  0.828066  4.3  5.1  5.80  6.4  7.9\n",
       "sepal width   150.0  3.057333  0.435866  2.0  2.8  3.00  3.3  4.4\n",
       "petal length  150.0  3.758000  1.765298  1.0  1.6  4.35  5.1  6.9\n",
       "petal width   150.0  1.199333  0.762238  0.1  0.3  1.30  1.8  2.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'data/bezdekIris.data'\n",
    "\n",
    "data = pd.read_csv(file, names=['sepal length', 'sepal width', 'petal length', 'petal width', 'class'])\n",
    "data['sepal length'] = data['sepal length'].astype(np.float32)\n",
    "data['sepal width'] = data['sepal width'].astype(np.float32)\n",
    "data['petal length'] = data['petal length'].astype(np.float32)\n",
    "data['petal width'] = data['petal width'].astype(np.float32)\n",
    "data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our values match what is provided in `data/iris.names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length  sepal width  petal length  petal width            class\n",
       "91            6.1          3.0           4.6          1.4  Iris-versicolor\n",
       "63            6.1          2.9           4.7          1.4  Iris-versicolor\n",
       "103           6.3          2.9           5.6          1.8   Iris-virginica\n",
       "6             4.6          3.4           1.4          0.3      Iris-setosa\n",
       "59            5.2          2.7           3.9          1.4  Iris-versicolor\n",
       "29            4.7          3.2           1.6          0.2      Iris-setosa\n",
       "27            5.2          3.5           1.5          0.2      Iris-setosa\n",
       "35            5.0          3.2           1.2          0.2      Iris-setosa\n",
       "99            5.7          2.8           4.1          1.3  Iris-versicolor\n",
       "122           7.7          2.8           6.7          2.0   Iris-virginica"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[['sepal length', 'sepal width', 'petal length', 'petal width']]\n",
    "x_train = x_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_classes = data['class'].unique().tolist()\n",
    "unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_count = len(unique_classes)\n",
    "class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_int = {}\n",
    "for i, item in enumerate(unique_classes):\n",
    "    class_to_int[item] = i\n",
    "class_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iris-versicolor'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def int_to_class(i):\n",
    "    return unique_classes[i]\n",
    "int_to_class(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_index = []\n",
    "y_onehot = []\n",
    "\n",
    "for item in data[['class']].values:\n",
    "    item = item[0]\n",
    "    \n",
    "    vec = np.zeros(class_count)\n",
    "    index = class_to_int[item]\n",
    "    vec[index] = 1\n",
    "    \n",
    "    y_index.append(index)\n",
    "    y_onehot.append(vec)\n",
    "\n",
    "data['index'] = y_index\n",
    "data['onehot'] = y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "      <th>class</th>\n",
       "      <th>index</th>\n",
       "      <th>onehot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>6.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length  sepal width  petal length  petal width            class  \\\n",
       "76            6.8          2.8           4.8          1.4  Iris-versicolor   \n",
       "105           7.6          3.0           6.6          2.1   Iris-virginica   \n",
       "73            6.1          2.8           4.7          1.2  Iris-versicolor   \n",
       "117           7.7          3.8           6.7          2.2   Iris-virginica   \n",
       "115           6.4          3.2           5.3          2.3   Iris-virginica   \n",
       "128           6.4          2.8           5.6          2.1   Iris-virginica   \n",
       "15            5.7          4.4           1.5          0.4      Iris-setosa   \n",
       "18            5.7          3.8           1.7          0.3      Iris-setosa   \n",
       "55            5.7          2.8           4.5          1.3  Iris-versicolor   \n",
       "77            6.7          3.0           5.0          1.7  Iris-versicolor   \n",
       "\n",
       "     index           onehot  \n",
       "76       1  [0.0, 1.0, 0.0]  \n",
       "105      2  [0.0, 0.0, 1.0]  \n",
       "73       1  [0.0, 1.0, 0.0]  \n",
       "117      2  [0.0, 0.0, 1.0]  \n",
       "115      2  [0.0, 0.0, 1.0]  \n",
       "128      2  [0.0, 0.0, 1.0]  \n",
       "15       0  [1.0, 0.0, 0.0]  \n",
       "18       0  [1.0, 0.0, 0.0]  \n",
       "55       1  [0.0, 1.0, 0.0]  \n",
       "77       1  [0.0, 1.0, 0.0]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCHS = 1024\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1022 15:17:06.679523  5476 deprecation.py:506] From c:\\users\\august\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1022 15:17:06.881071  5476 deprecation.py:506] From c:\\users\\august\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, 4))\n",
    "y = tf.placeholder(tf.float32, shape=(None, class_count))\n",
    "\n",
    "out = tf.layers.Dense(3, activation='linear', kernel_initializer='uniform', use_bias=False)(x)\n",
    "logits = tf.layers.Dense(class_count)(out)\n",
    "probs = tf.nn.softmax(logits=logits)\n",
    "\n",
    "rng = np.random.RandomState([2019, 10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1022 15:17:06.925168  5476 deprecation.py:323] From c:\\users\\august\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.softmax_cross_entropy(y, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.train.GradientDescentOptimizer(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 1.1175056  Acc 0.3333333333333333  \n",
      "Epoch 1  Loss 1.1162529  Acc 0.3333333333333333  \n",
      "Epoch 2  Loss 1.1150053  Acc 0.3333333333333333  \n",
      "Epoch 3  Loss 1.1136588  Acc 0.3333333333333333  \n",
      "Epoch 4  Loss 1.112493  Acc 0.3333333333333333  \n",
      "Epoch 5  Loss 1.111351  Acc 0.3333333333333333  \n",
      "Epoch 6  Loss 1.1100862  Acc 0.3333333333333333  \n",
      "Epoch 7  Loss 1.1089019  Acc 0.3333333333333333  \n",
      "Epoch 8  Loss 1.1076524  Acc 0.32666666666666666  \n",
      "Epoch 9  Loss 1.1064526  Acc 0.32666666666666666  \n",
      "Epoch 10  Loss 1.1054059  Acc 0.32666666666666666  \n",
      "Epoch 11  Loss 1.1044073  Acc 0.29333333333333333  \n",
      "Epoch 12  Loss 1.1034153  Acc 0.29333333333333333  \n",
      "Epoch 13  Loss 1.1023979  Acc 0.24  \n",
      "Epoch 14  Loss 1.1014053  Acc 0.20666666666666667  \n",
      "Epoch 15  Loss 1.1003796  Acc 0.13333333333333333  \n",
      "Epoch 16  Loss 1.099408  Acc 0.08666666666666667  \n",
      "Epoch 17  Loss 1.0984799  Acc 0.06  \n",
      "Epoch 18  Loss 1.0975082  Acc 0.03333333333333333  \n",
      "Epoch 19  Loss 1.0966029  Acc 0.013333333333333334  \n",
      "Epoch 20  Loss 1.0956327  Acc 0.02  \n",
      "Epoch 21  Loss 1.094766  Acc 0.07333333333333333  \n",
      "Epoch 22  Loss 1.0939251  Acc 0.14666666666666667  \n",
      "Epoch 23  Loss 1.0931263  Acc 0.32666666666666666  \n",
      "Epoch 24  Loss 1.0922952  Acc 0.5266666666666666  \n",
      "Epoch 25  Loss 1.091409  Acc 0.6  \n",
      "Epoch 26  Loss 1.0905648  Acc 0.5466666666666666  \n",
      "Epoch 27  Loss 1.0896267  Acc 0.46  \n",
      "Epoch 28  Loss 1.0888362  Acc 0.4  \n",
      "Epoch 29  Loss 1.0879663  Acc 0.3466666666666667  \n",
      "Epoch 30  Loss 1.0871558  Acc 0.3466666666666667  \n",
      "Epoch 31  Loss 1.0863519  Acc 0.3333333333333333  \n",
      "Epoch 32  Loss 1.0855632  Acc 0.3333333333333333  \n",
      "Epoch 33  Loss 1.0847985  Acc 0.3333333333333333  \n",
      "Epoch 34  Loss 1.0839773  Acc 0.3333333333333333  \n",
      "Epoch 35  Loss 1.0831842  Acc 0.3333333333333333  \n",
      "Epoch 36  Loss 1.0824342  Acc 0.3333333333333333  \n",
      "Epoch 37  Loss 1.0817506  Acc 0.3333333333333333  \n",
      "Epoch 38  Loss 1.0810603  Acc 0.3333333333333333  \n",
      "Epoch 39  Loss 1.080338  Acc 0.3333333333333333  \n",
      "Epoch 40  Loss 1.079698  Acc 0.3333333333333333  \n",
      "Epoch 41  Loss 1.0790298  Acc 0.3333333333333333  \n",
      "Epoch 42  Loss 1.0783509  Acc 0.3333333333333333  \n",
      "Epoch 43  Loss 1.0777161  Acc 0.3333333333333333  \n",
      "Epoch 44  Loss 1.0769998  Acc 0.3333333333333333  \n",
      "Epoch 45  Loss 1.076349  Acc 0.3333333333333333  \n",
      "Epoch 46  Loss 1.0756766  Acc 0.3333333333333333  \n",
      "Epoch 47  Loss 1.0750502  Acc 0.3333333333333333  \n",
      "Epoch 48  Loss 1.0744184  Acc 0.3333333333333333  \n",
      "Epoch 49  Loss 1.0738044  Acc 0.3333333333333333  \n",
      "Epoch 50  Loss 1.073077  Acc 0.3333333333333333  \n",
      "Epoch 51  Loss 1.0724156  Acc 0.3333333333333333  \n",
      "Epoch 52  Loss 1.0717112  Acc 0.3333333333333333  \n",
      "Epoch 53  Loss 1.0711296  Acc 0.3333333333333333  \n",
      "Epoch 54  Loss 1.0705265  Acc 0.3333333333333333  \n",
      "Epoch 55  Loss 1.069905  Acc 0.3333333333333333  \n",
      "Epoch 56  Loss 1.0692877  Acc 0.3333333333333333  \n",
      "Epoch 57  Loss 1.0687572  Acc 0.3333333333333333  \n",
      "Epoch 58  Loss 1.068144  Acc 0.3333333333333333  \n",
      "Epoch 59  Loss 1.0675676  Acc 0.3333333333333333  \n",
      "Epoch 60  Loss 1.0670186  Acc 0.3333333333333333  \n",
      "Epoch 61  Loss 1.0665107  Acc 0.3333333333333333  \n",
      "Epoch 62  Loss 1.0659435  Acc 0.3333333333333333  \n",
      "Epoch 63  Loss 1.0654104  Acc 0.3333333333333333  \n",
      "Epoch 64  Loss 1.0648695  Acc 0.3333333333333333  \n",
      "Epoch 65  Loss 1.0643198  Acc 0.3333333333333333  \n",
      "Epoch 66  Loss 1.063723  Acc 0.3333333333333333  \n",
      "Epoch 67  Loss 1.0632036  Acc 0.3333333333333333  \n",
      "Epoch 68  Loss 1.0626462  Acc 0.3333333333333333  \n",
      "Epoch 69  Loss 1.0620896  Acc 0.3333333333333333  \n",
      "Epoch 70  Loss 1.0615814  Acc 0.3333333333333333  \n",
      "Epoch 71  Loss 1.061041  Acc 0.3333333333333333  \n",
      "Epoch 72  Loss 1.0605594  Acc 0.3333333333333333  \n",
      "Epoch 73  Loss 1.060043  Acc 0.3333333333333333  \n",
      "Epoch 74  Loss 1.0595276  Acc 0.3333333333333333  \n",
      "Epoch 75  Loss 1.0589994  Acc 0.3333333333333333  \n",
      "Epoch 76  Loss 1.0585101  Acc 0.3333333333333333  \n",
      "Epoch 77  Loss 1.0580271  Acc 0.3333333333333333  \n",
      "Epoch 78  Loss 1.0575217  Acc 0.3333333333333333  \n",
      "Epoch 79  Loss 1.057059  Acc 0.3333333333333333  \n",
      "Epoch 80  Loss 1.056542  Acc 0.3333333333333333  \n",
      "Epoch 81  Loss 1.0560259  Acc 0.3333333333333333  \n",
      "Epoch 82  Loss 1.0555165  Acc 0.3333333333333333  \n",
      "Epoch 83  Loss 1.0550148  Acc 0.3333333333333333  \n",
      "Epoch 84  Loss 1.054509  Acc 0.3333333333333333  \n",
      "Epoch 85  Loss 1.0539895  Acc 0.3333333333333333  \n",
      "Epoch 86  Loss 1.0534991  Acc 0.3333333333333333  \n",
      "Epoch 87  Loss 1.0530264  Acc 0.3333333333333333  \n",
      "Epoch 88  Loss 1.0525559  Acc 0.3333333333333333  \n",
      "Epoch 89  Loss 1.0521015  Acc 0.3333333333333333  \n",
      "Epoch 90  Loss 1.0516496  Acc 0.3333333333333333  \n",
      "Epoch 91  Loss 1.0511783  Acc 0.3333333333333333  \n",
      "Epoch 92  Loss 1.05071  Acc 0.3333333333333333  \n",
      "Epoch 93  Loss 1.0502304  Acc 0.3333333333333333  \n",
      "Epoch 94  Loss 1.0497711  Acc 0.3333333333333333  \n",
      "Epoch 95  Loss 1.0493218  Acc 0.3333333333333333  \n",
      "Epoch 96  Loss 1.0488906  Acc 0.3333333333333333  \n",
      "Epoch 97  Loss 1.0484608  Acc 0.3333333333333333  \n",
      "Epoch 98  Loss 1.0480014  Acc 0.3333333333333333  \n",
      "Epoch 99  Loss 1.0475456  Acc 0.3333333333333333  \n",
      "Epoch 100  Loss 1.0470834  Acc 0.3333333333333333  \n",
      "Epoch 101  Loss 1.0466044  Acc 0.3333333333333333  \n",
      "Epoch 102  Loss 1.0461416  Acc 0.3333333333333333  \n",
      "Epoch 103  Loss 1.0457077  Acc 0.3333333333333333  \n",
      "Epoch 104  Loss 1.0452746  Acc 0.3333333333333333  \n",
      "Epoch 105  Loss 1.0448433  Acc 0.3333333333333333  \n",
      "Epoch 106  Loss 1.0444034  Acc 0.3333333333333333  \n",
      "Epoch 107  Loss 1.0439631  Acc 0.3333333333333333  \n",
      "Epoch 108  Loss 1.043506  Acc 0.3333333333333333  \n",
      "Epoch 109  Loss 1.0430797  Acc 0.3333333333333333  \n",
      "Epoch 110  Loss 1.0426435  Acc 0.3333333333333333  \n",
      "Epoch 111  Loss 1.042218  Acc 0.3333333333333333  \n",
      "Epoch 112  Loss 1.0417855  Acc 0.3333333333333333  \n",
      "Epoch 113  Loss 1.0413281  Acc 0.3333333333333333  \n",
      "Epoch 114  Loss 1.040883  Acc 0.3333333333333333  \n",
      "Epoch 115  Loss 1.0404432  Acc 0.3333333333333333  \n",
      "Epoch 116  Loss 1.0400369  Acc 0.3333333333333333  \n",
      "Epoch 117  Loss 1.0395907  Acc 0.3333333333333333  \n",
      "Epoch 118  Loss 1.0391746  Acc 0.3333333333333333  \n",
      "Epoch 119  Loss 1.0387701  Acc 0.3333333333333333  \n",
      "Epoch 120  Loss 1.0383403  Acc 0.3333333333333333  \n",
      "Epoch 121  Loss 1.0379049  Acc 0.3333333333333333  \n",
      "Epoch 122  Loss 1.0374827  Acc 0.3333333333333333  \n",
      "Epoch 123  Loss 1.037057  Acc 0.3333333333333333  \n",
      "Epoch 124  Loss 1.0366552  Acc 0.3333333333333333  \n",
      "Epoch 125  Loss 1.0362383  Acc 0.3333333333333333  \n",
      "Epoch 126  Loss 1.035815  Acc 0.3333333333333333  \n",
      "Epoch 127  Loss 1.0354067  Acc 0.3333333333333333  \n",
      "Epoch 128  Loss 1.0349826  Acc 0.3333333333333333  \n",
      "Epoch 129  Loss 1.034567  Acc 0.3333333333333333  \n",
      "Epoch 130  Loss 1.0341517  Acc 0.3333333333333333  \n",
      "Epoch 131  Loss 1.03372  Acc 0.3333333333333333  \n",
      "Epoch 132  Loss 1.0333197  Acc 0.3333333333333333  \n",
      "Epoch 133  Loss 1.0329092  Acc 0.3333333333333333  \n",
      "Epoch 134  Loss 1.0324914  Acc 0.3333333333333333  \n",
      "Epoch 135  Loss 1.0320671  Acc 0.3333333333333333  \n",
      "Epoch 136  Loss 1.0316517  Acc 0.3333333333333333  \n",
      "Epoch 137  Loss 1.0312399  Acc 0.3333333333333333  \n",
      "Epoch 138  Loss 1.0308284  Acc 0.3333333333333333  \n",
      "Epoch 139  Loss 1.0304084  Acc 0.3333333333333333  \n",
      "Epoch 140  Loss 1.0300046  Acc 0.3333333333333333  \n",
      "Epoch 141  Loss 1.0295855  Acc 0.3333333333333333  \n",
      "Epoch 142  Loss 1.0291748  Acc 0.3333333333333333  \n",
      "Epoch 143  Loss 1.028762  Acc 0.3333333333333333  \n",
      "Epoch 144  Loss 1.0283605  Acc 0.3333333333333333  \n",
      "Epoch 145  Loss 1.0279483  Acc 0.3333333333333333  \n",
      "Epoch 146  Loss 1.0275464  Acc 0.3333333333333333  \n",
      "Epoch 147  Loss 1.0271302  Acc 0.3333333333333333  \n",
      "Epoch 148  Loss 1.0267199  Acc 0.3333333333333333  \n",
      "Epoch 149  Loss 1.026314  Acc 0.3333333333333333  \n",
      "Epoch 150  Loss 1.0259198  Acc 0.3333333333333333  \n",
      "Epoch 151  Loss 1.0255208  Acc 0.3333333333333333  \n",
      "Epoch 152  Loss 1.0251229  Acc 0.3333333333333333  \n",
      "Epoch 153  Loss 1.0247222  Acc 0.3333333333333333  \n",
      "Epoch 154  Loss 1.024306  Acc 0.3333333333333333  \n",
      "Epoch 155  Loss 1.0239028  Acc 0.3333333333333333  \n",
      "Epoch 156  Loss 1.0234886  Acc 0.3333333333333333  \n",
      "Epoch 157  Loss 1.023091  Acc 0.3333333333333333  \n",
      "Epoch 158  Loss 1.0226977  Acc 0.3333333333333333  \n",
      "Epoch 159  Loss 1.0223026  Acc 0.3333333333333333  \n",
      "Epoch 160  Loss 1.0218962  Acc 0.3333333333333333  \n",
      "Epoch 161  Loss 1.0215125  Acc 0.3333333333333333  \n",
      "Epoch 162  Loss 1.0211214  Acc 0.3333333333333333  \n",
      "Epoch 163  Loss 1.0207251  Acc 0.3333333333333333  \n",
      "Epoch 164  Loss 1.0203243  Acc 0.3333333333333333  \n",
      "Epoch 165  Loss 1.0199406  Acc 0.3333333333333333  \n",
      "Epoch 166  Loss 1.019549  Acc 0.3333333333333333  \n",
      "Epoch 167  Loss 1.0191588  Acc 0.3333333333333333  \n",
      "Epoch 168  Loss 1.0187659  Acc 0.3333333333333333  \n",
      "Epoch 169  Loss 1.0183685  Acc 0.3333333333333333  \n",
      "Epoch 170  Loss 1.0179722  Acc 0.3333333333333333  \n",
      "Epoch 171  Loss 1.0175793  Acc 0.3333333333333333  \n",
      "Epoch 172  Loss 1.017192  Acc 0.3333333333333333  \n",
      "Epoch 173  Loss 1.0168072  Acc 0.3333333333333333  \n",
      "Epoch 174  Loss 1.0164152  Acc 0.3333333333333333  \n",
      "Epoch 175  Loss 1.0160325  Acc 0.3333333333333333  \n",
      "Epoch 176  Loss 1.0156435  Acc 0.3333333333333333  \n",
      "Epoch 177  Loss 1.0152451  Acc 0.3333333333333333  \n",
      "Epoch 178  Loss 1.0148507  Acc 0.3333333333333333  \n",
      "Epoch 179  Loss 1.0144668  Acc 0.3333333333333333  \n",
      "Epoch 180  Loss 1.0140715  Acc 0.3333333333333333  \n",
      "Epoch 181  Loss 1.0136861  Acc 0.3333333333333333  \n",
      "Epoch 182  Loss 1.0132961  Acc 0.3333333333333333  \n",
      "Epoch 183  Loss 1.0129174  Acc 0.3333333333333333  \n",
      "Epoch 184  Loss 1.0125272  Acc 0.3333333333333333  \n",
      "Epoch 185  Loss 1.012139  Acc 0.3333333333333333  \n",
      "Epoch 186  Loss 1.0117469  Acc 0.3333333333333333  \n",
      "Epoch 187  Loss 1.011363  Acc 0.3333333333333333  \n",
      "Epoch 188  Loss 1.0109798  Acc 0.3333333333333333  \n",
      "Epoch 189  Loss 1.0105976  Acc 0.3333333333333333  \n",
      "Epoch 190  Loss 1.0102043  Acc 0.3333333333333333  \n",
      "Epoch 191  Loss 1.0098194  Acc 0.34  \n",
      "Epoch 192  Loss 1.0094374  Acc 0.34  \n",
      "Epoch 193  Loss 1.0090532  Acc 0.3466666666666667  \n",
      "Epoch 194  Loss 1.0086704  Acc 0.3466666666666667  \n",
      "Epoch 195  Loss 1.0082902  Acc 0.3466666666666667  \n",
      "Epoch 196  Loss 1.0079049  Acc 0.3466666666666667  \n",
      "Epoch 197  Loss 1.007513  Acc 0.3466666666666667  \n",
      "Epoch 198  Loss 1.0071263  Acc 0.3466666666666667  \n",
      "Epoch 199  Loss 1.0067408  Acc 0.3466666666666667  \n",
      "Epoch 200  Loss 1.0063461  Acc 0.3466666666666667  \n",
      "Epoch 201  Loss 1.0059628  Acc 0.3466666666666667  \n",
      "Epoch 202  Loss 1.0055879  Acc 0.3466666666666667  \n",
      "Epoch 203  Loss 1.0051998  Acc 0.3466666666666667  \n",
      "Epoch 204  Loss 1.0048124  Acc 0.3466666666666667  \n",
      "Epoch 205  Loss 1.0044324  Acc 0.3466666666666667  \n",
      "Epoch 206  Loss 1.0040473  Acc 0.3466666666666667  \n",
      "Epoch 207  Loss 1.0036616  Acc 0.3466666666666667  \n",
      "Epoch 208  Loss 1.0032773  Acc 0.3466666666666667  \n",
      "Epoch 209  Loss 1.0029  Acc 0.3466666666666667  \n",
      "Epoch 210  Loss 1.002506  Acc 0.35333333333333333  \n",
      "Epoch 211  Loss 1.002119  Acc 0.35333333333333333  \n",
      "Epoch 212  Loss 1.0017408  Acc 0.35333333333333333  \n",
      "Epoch 213  Loss 1.0013543  Acc 0.36666666666666664  \n",
      "Epoch 214  Loss 1.0009724  Acc 0.37333333333333335  \n",
      "Epoch 215  Loss 1.000592  Acc 0.37333333333333335  \n",
      "Epoch 216  Loss 1.0002104  Acc 0.38  \n",
      "Epoch 217  Loss 0.9998349  Acc 0.38  \n",
      "Epoch 218  Loss 0.9994517  Acc 0.38  \n",
      "Epoch 219  Loss 0.999064  Acc 0.38666666666666666  \n",
      "Epoch 220  Loss 0.99868816  Acc 0.38666666666666666  \n",
      "Epoch 221  Loss 0.99831176  Acc 0.38666666666666666  \n",
      "Epoch 222  Loss 0.9979317  Acc 0.3933333333333333  \n",
      "Epoch 223  Loss 0.9975534  Acc 0.3933333333333333  \n",
      "Epoch 224  Loss 0.9971797  Acc 0.3933333333333333  \n",
      "Epoch 225  Loss 0.99680185  Acc 0.3933333333333333  \n",
      "Epoch 226  Loss 0.9964265  Acc 0.3933333333333333  \n",
      "Epoch 227  Loss 0.99604565  Acc 0.3933333333333333  \n",
      "Epoch 228  Loss 0.9956642  Acc 0.4  \n",
      "Epoch 229  Loss 0.99529094  Acc 0.4  \n",
      "Epoch 230  Loss 0.99490887  Acc 0.4  \n",
      "Epoch 231  Loss 0.9945215  Acc 0.42  \n",
      "Epoch 232  Loss 0.9941377  Acc 0.44  \n",
      "Epoch 233  Loss 0.99376035  Acc 0.44666666666666666  \n",
      "Epoch 234  Loss 0.993378  Acc 0.4533333333333333  \n",
      "Epoch 235  Loss 0.9930015  Acc 0.4533333333333333  \n",
      "Epoch 236  Loss 0.99261975  Acc 0.4666666666666667  \n",
      "Epoch 237  Loss 0.99225515  Acc 0.47333333333333333  \n",
      "Epoch 238  Loss 0.9918868  Acc 0.47333333333333333  \n",
      "Epoch 239  Loss 0.99150735  Acc 0.47333333333333333  \n",
      "Epoch 240  Loss 0.99114025  Acc 0.47333333333333333  \n",
      "Epoch 241  Loss 0.9907606  Acc 0.47333333333333333  \n",
      "Epoch 242  Loss 0.99039173  Acc 0.47333333333333333  \n",
      "Epoch 243  Loss 0.9900122  Acc 0.4866666666666667  \n",
      "Epoch 244  Loss 0.98964274  Acc 0.4866666666666667  \n",
      "Epoch 245  Loss 0.98926085  Acc 0.5  \n",
      "Epoch 246  Loss 0.9888798  Acc 0.5066666666666667  \n",
      "Epoch 247  Loss 0.98850065  Acc 0.5066666666666667  \n",
      "Epoch 248  Loss 0.9881167  Acc 0.5133333333333333  \n",
      "Epoch 249  Loss 0.9877404  Acc 0.5133333333333333  \n",
      "Epoch 250  Loss 0.9873637  Acc 0.54  \n",
      "Epoch 251  Loss 0.986993  Acc 0.5466666666666666  \n",
      "Epoch 252  Loss 0.98661375  Acc 0.5466666666666666  \n",
      "Epoch 253  Loss 0.9862366  Acc 0.5466666666666666  \n",
      "Epoch 254  Loss 0.9858715  Acc 0.5466666666666666  \n",
      "Epoch 255  Loss 0.9854877  Acc 0.56  \n",
      "Epoch 256  Loss 0.98511046  Acc 0.56  \n",
      "Epoch 257  Loss 0.9847419  Acc 0.56  \n",
      "Epoch 258  Loss 0.984374  Acc 0.56  \n",
      "Epoch 259  Loss 0.9840035  Acc 0.56  \n",
      "Epoch 260  Loss 0.98363465  Acc 0.56  \n",
      "Epoch 261  Loss 0.98326784  Acc 0.5666666666666667  \n",
      "Epoch 262  Loss 0.9828999  Acc 0.5666666666666667  \n",
      "Epoch 263  Loss 0.982536  Acc 0.5666666666666667  \n",
      "Epoch 264  Loss 0.98217684  Acc 0.5666666666666667  \n",
      "Epoch 265  Loss 0.9818016  Acc 0.5733333333333334  \n",
      "Epoch 266  Loss 0.9814387  Acc 0.5733333333333334  \n",
      "Epoch 267  Loss 0.98105955  Acc 0.58  \n",
      "Epoch 268  Loss 0.98068136  Acc 0.58  \n",
      "Epoch 269  Loss 0.9803074  Acc 0.58  \n",
      "Epoch 270  Loss 0.97993803  Acc 0.5866666666666667  \n",
      "Epoch 271  Loss 0.97957456  Acc 0.5866666666666667  \n",
      "Epoch 272  Loss 0.97919697  Acc 0.5866666666666667  \n",
      "Epoch 273  Loss 0.978826  Acc 0.5866666666666667  \n",
      "Epoch 274  Loss 0.9784431  Acc 0.6133333333333333  \n",
      "Epoch 275  Loss 0.9780698  Acc 0.62  \n",
      "Epoch 276  Loss 0.9777105  Acc 0.62  \n",
      "Epoch 277  Loss 0.9773352  Acc 0.62  \n",
      "Epoch 278  Loss 0.9769572  Acc 0.6266666666666667  \n",
      "Epoch 279  Loss 0.97659564  Acc 0.6266666666666667  \n",
      "Epoch 280  Loss 0.9762261  Acc 0.6266666666666667  \n",
      "Epoch 281  Loss 0.975856  Acc 0.6266666666666667  \n",
      "Epoch 282  Loss 0.97548485  Acc 0.6266666666666667  \n",
      "Epoch 283  Loss 0.9751148  Acc 0.6266666666666667  \n",
      "Epoch 284  Loss 0.97474724  Acc 0.6266666666666667  \n",
      "Epoch 285  Loss 0.9743808  Acc 0.6333333333333333  \n",
      "Epoch 286  Loss 0.97401226  Acc 0.64  \n",
      "Epoch 287  Loss 0.97363526  Acc 0.6466666666666666  \n",
      "Epoch 288  Loss 0.973264  Acc 0.6466666666666666  \n",
      "Epoch 289  Loss 0.97289044  Acc 0.6466666666666666  \n",
      "Epoch 290  Loss 0.9725212  Acc 0.6466666666666666  \n",
      "Epoch 291  Loss 0.97214496  Acc 0.6466666666666666  \n",
      "Epoch 292  Loss 0.971775  Acc 0.6466666666666666  \n",
      "Epoch 293  Loss 0.9713972  Acc 0.6533333333333333  \n",
      "Epoch 294  Loss 0.9710251  Acc 0.6533333333333333  \n",
      "Epoch 295  Loss 0.97066855  Acc 0.6533333333333333  \n",
      "Epoch 296  Loss 0.97029907  Acc 0.6533333333333333  \n",
      "Epoch 297  Loss 0.96993244  Acc 0.6533333333333333  \n",
      "Epoch 298  Loss 0.9695633  Acc 0.6533333333333333  \n",
      "Epoch 299  Loss 0.9691899  Acc 0.6533333333333333  \n",
      "Epoch 300  Loss 0.96881735  Acc 0.6533333333333333  \n",
      "Epoch 301  Loss 0.968444  Acc 0.66  \n",
      "Epoch 302  Loss 0.9680723  Acc 0.66  \n",
      "Epoch 303  Loss 0.967711  Acc 0.66  \n",
      "Epoch 304  Loss 0.96735126  Acc 0.66  \n",
      "Epoch 305  Loss 0.9669895  Acc 0.66  \n",
      "Epoch 306  Loss 0.966628  Acc 0.66  \n",
      "Epoch 307  Loss 0.96626127  Acc 0.66  \n",
      "Epoch 308  Loss 0.96589833  Acc 0.66  \n",
      "Epoch 309  Loss 0.96553844  Acc 0.66  \n",
      "Epoch 310  Loss 0.96516746  Acc 0.66  \n",
      "Epoch 311  Loss 0.9647902  Acc 0.66  \n",
      "Epoch 312  Loss 0.9644293  Acc 0.66  \n",
      "Epoch 313  Loss 0.96406484  Acc 0.66  \n",
      "Epoch 314  Loss 0.96369946  Acc 0.66  \n",
      "Epoch 315  Loss 0.9633374  Acc 0.66  \n",
      "Epoch 316  Loss 0.96297127  Acc 0.66  \n",
      "Epoch 317  Loss 0.9626117  Acc 0.66  \n",
      "Epoch 318  Loss 0.9622515  Acc 0.66  \n",
      "Epoch 319  Loss 0.9618807  Acc 0.66  \n",
      "Epoch 320  Loss 0.96151  Acc 0.66  \n",
      "Epoch 321  Loss 0.96114504  Acc 0.66  \n",
      "Epoch 322  Loss 0.960779  Acc 0.66  \n",
      "Epoch 323  Loss 0.9604158  Acc 0.66  \n",
      "Epoch 324  Loss 0.96005476  Acc 0.66  \n",
      "Epoch 325  Loss 0.9596929  Acc 0.66  \n",
      "Epoch 326  Loss 0.95932657  Acc 0.6666666666666666  \n",
      "Epoch 327  Loss 0.95896095  Acc 0.6666666666666666  \n",
      "Epoch 328  Loss 0.95860434  Acc 0.6666666666666666  \n",
      "Epoch 329  Loss 0.95823914  Acc 0.6666666666666666  \n",
      "Epoch 330  Loss 0.957884  Acc 0.6666666666666666  \n",
      "Epoch 331  Loss 0.95751894  Acc 0.6666666666666666  \n",
      "Epoch 332  Loss 0.95715314  Acc 0.6666666666666666  \n",
      "Epoch 333  Loss 0.9567953  Acc 0.6666666666666666  \n",
      "Epoch 334  Loss 0.9564343  Acc 0.6666666666666666  \n",
      "Epoch 335  Loss 0.9560787  Acc 0.6666666666666666  \n",
      "Epoch 336  Loss 0.9557134  Acc 0.6666666666666666  \n",
      "Epoch 337  Loss 0.9553538  Acc 0.6666666666666666  \n",
      "Epoch 338  Loss 0.954993  Acc 0.6666666666666666  \n",
      "Epoch 339  Loss 0.9546313  Acc 0.6666666666666666  \n",
      "Epoch 340  Loss 0.9542651  Acc 0.6666666666666666  \n",
      "Epoch 341  Loss 0.95390093  Acc 0.6666666666666666  \n",
      "Epoch 342  Loss 0.95352155  Acc 0.6666666666666666  \n",
      "Epoch 343  Loss 0.953149  Acc 0.6666666666666666  \n",
      "Epoch 344  Loss 0.95278585  Acc 0.6666666666666666  \n",
      "Epoch 345  Loss 0.95242953  Acc 0.6666666666666666  \n",
      "Epoch 346  Loss 0.9520632  Acc 0.6666666666666666  \n",
      "Epoch 347  Loss 0.9517061  Acc 0.6666666666666666  \n",
      "Epoch 348  Loss 0.9513451  Acc 0.6666666666666666  \n",
      "Epoch 349  Loss 0.9509921  Acc 0.6666666666666666  \n",
      "Epoch 350  Loss 0.95062083  Acc 0.6666666666666666  \n",
      "Epoch 351  Loss 0.95025927  Acc 0.6666666666666666  \n",
      "Epoch 352  Loss 0.94989043  Acc 0.6666666666666666  \n",
      "Epoch 353  Loss 0.9495442  Acc 0.6666666666666666  \n",
      "Epoch 354  Loss 0.9491812  Acc 0.6666666666666666  \n",
      "Epoch 355  Loss 0.9488134  Acc 0.6666666666666666  \n",
      "Epoch 356  Loss 0.94846404  Acc 0.6666666666666666  \n",
      "Epoch 357  Loss 0.9481138  Acc 0.6666666666666666  \n",
      "Epoch 358  Loss 0.9477552  Acc 0.6666666666666666  \n",
      "Epoch 359  Loss 0.9474006  Acc 0.6666666666666666  \n",
      "Epoch 360  Loss 0.94703615  Acc 0.6666666666666666  \n",
      "Epoch 361  Loss 0.9466748  Acc 0.6666666666666666  \n",
      "Epoch 362  Loss 0.94630605  Acc 0.6666666666666666  \n",
      "Epoch 363  Loss 0.9459491  Acc 0.6666666666666666  \n",
      "Epoch 364  Loss 0.94558704  Acc 0.6666666666666666  \n",
      "Epoch 365  Loss 0.945227  Acc 0.6666666666666666  \n",
      "Epoch 366  Loss 0.944869  Acc 0.6666666666666666  \n",
      "Epoch 367  Loss 0.9445088  Acc 0.6666666666666666  \n",
      "Epoch 368  Loss 0.94414604  Acc 0.6666666666666666  \n",
      "Epoch 369  Loss 0.94377726  Acc 0.6666666666666666  \n",
      "Epoch 370  Loss 0.9434296  Acc 0.6666666666666666  \n",
      "Epoch 371  Loss 0.943072  Acc 0.6666666666666666  \n",
      "Epoch 372  Loss 0.942711  Acc 0.6666666666666666  \n",
      "Epoch 373  Loss 0.94233966  Acc 0.6666666666666666  \n",
      "Epoch 374  Loss 0.9419828  Acc 0.6666666666666666  \n",
      "Epoch 375  Loss 0.94162333  Acc 0.6666666666666666  \n",
      "Epoch 376  Loss 0.9412584  Acc 0.6666666666666666  \n",
      "Epoch 377  Loss 0.9409015  Acc 0.6666666666666666  \n",
      "Epoch 378  Loss 0.9405404  Acc 0.6666666666666666  \n",
      "Epoch 379  Loss 0.9401811  Acc 0.6666666666666666  \n",
      "Epoch 380  Loss 0.93982565  Acc 0.6666666666666666  \n",
      "Epoch 381  Loss 0.9394752  Acc 0.6666666666666666  \n",
      "Epoch 382  Loss 0.9391088  Acc 0.6666666666666666  \n",
      "Epoch 383  Loss 0.9387476  Acc 0.6666666666666666  \n",
      "Epoch 384  Loss 0.9383889  Acc 0.6666666666666666  \n",
      "Epoch 385  Loss 0.9380329  Acc 0.6666666666666666  \n",
      "Epoch 386  Loss 0.9376779  Acc 0.6666666666666666  \n",
      "Epoch 387  Loss 0.9373205  Acc 0.6666666666666666  \n",
      "Epoch 388  Loss 0.93696195  Acc 0.6666666666666666  \n",
      "Epoch 389  Loss 0.9366143  Acc 0.6666666666666666  \n",
      "Epoch 390  Loss 0.9362593  Acc 0.6666666666666666  \n",
      "Epoch 391  Loss 0.9358941  Acc 0.6666666666666666  \n",
      "Epoch 392  Loss 0.9355245  Acc 0.6666666666666666  \n",
      "Epoch 393  Loss 0.9351601  Acc 0.6666666666666666  \n",
      "Epoch 394  Loss 0.9348008  Acc 0.6666666666666666  \n",
      "Epoch 395  Loss 0.9344396  Acc 0.6666666666666666  \n",
      "Epoch 396  Loss 0.934077  Acc 0.6666666666666666  \n",
      "Epoch 397  Loss 0.9337197  Acc 0.6666666666666666  \n",
      "Epoch 398  Loss 0.93336433  Acc 0.6666666666666666  \n",
      "Epoch 399  Loss 0.93300414  Acc 0.6666666666666666  \n",
      "Epoch 400  Loss 0.93265414  Acc 0.6666666666666666  \n",
      "Epoch 401  Loss 0.9322966  Acc 0.6666666666666666  \n",
      "Epoch 402  Loss 0.9319452  Acc 0.6666666666666666  \n",
      "Epoch 403  Loss 0.9315987  Acc 0.6666666666666666  \n",
      "Epoch 404  Loss 0.93124014  Acc 0.6666666666666666  \n",
      "Epoch 405  Loss 0.9308856  Acc 0.6666666666666666  \n",
      "Epoch 406  Loss 0.9305266  Acc 0.6666666666666666  \n",
      "Epoch 407  Loss 0.93016803  Acc 0.6666666666666666  \n",
      "Epoch 408  Loss 0.9298006  Acc 0.6666666666666666  \n",
      "Epoch 409  Loss 0.92945045  Acc 0.6666666666666666  \n",
      "Epoch 410  Loss 0.9290979  Acc 0.6666666666666666  \n",
      "Epoch 411  Loss 0.9287383  Acc 0.6666666666666666  \n",
      "Epoch 412  Loss 0.9283848  Acc 0.6666666666666666  \n",
      "Epoch 413  Loss 0.92803395  Acc 0.6666666666666666  \n",
      "Epoch 414  Loss 0.92768055  Acc 0.6666666666666666  \n",
      "Epoch 415  Loss 0.9273195  Acc 0.6666666666666666  \n",
      "Epoch 416  Loss 0.92695993  Acc 0.6666666666666666  \n",
      "Epoch 417  Loss 0.9266028  Acc 0.6666666666666666  \n",
      "Epoch 418  Loss 0.926256  Acc 0.6666666666666666  \n",
      "Epoch 419  Loss 0.9259012  Acc 0.6666666666666666  \n",
      "Epoch 420  Loss 0.92553496  Acc 0.6666666666666666  \n",
      "Epoch 421  Loss 0.9251794  Acc 0.6666666666666666  \n",
      "Epoch 422  Loss 0.9247991  Acc 0.6666666666666666  \n",
      "Epoch 423  Loss 0.9244418  Acc 0.6666666666666666  \n",
      "Epoch 424  Loss 0.9240783  Acc 0.6666666666666666  \n",
      "Epoch 425  Loss 0.9237271  Acc 0.6666666666666666  \n",
      "Epoch 426  Loss 0.9233761  Acc 0.6666666666666666  \n",
      "Epoch 427  Loss 0.92302793  Acc 0.6666666666666666  \n",
      "Epoch 428  Loss 0.92266756  Acc 0.6666666666666666  \n",
      "Epoch 429  Loss 0.9223238  Acc 0.6666666666666666  \n",
      "Epoch 430  Loss 0.92196786  Acc 0.6666666666666666  \n",
      "Epoch 431  Loss 0.9216215  Acc 0.6666666666666666  \n",
      "Epoch 432  Loss 0.9212665  Acc 0.6666666666666666  \n",
      "Epoch 433  Loss 0.9209114  Acc 0.6666666666666666  \n",
      "Epoch 434  Loss 0.9205561  Acc 0.6666666666666666  \n",
      "Epoch 435  Loss 0.9202079  Acc 0.6666666666666666  \n",
      "Epoch 436  Loss 0.9198572  Acc 0.6666666666666666  \n",
      "Epoch 437  Loss 0.9195108  Acc 0.6666666666666666  \n",
      "Epoch 438  Loss 0.9191575  Acc 0.6666666666666666  \n",
      "Epoch 439  Loss 0.91880363  Acc 0.6666666666666666  \n",
      "Epoch 440  Loss 0.9184485  Acc 0.6666666666666666  \n",
      "Epoch 441  Loss 0.918097  Acc 0.6666666666666666  \n",
      "Epoch 442  Loss 0.91775036  Acc 0.6666666666666666  \n",
      "Epoch 443  Loss 0.9174062  Acc 0.6666666666666666  \n",
      "Epoch 444  Loss 0.9170465  Acc 0.6666666666666666  \n",
      "Epoch 445  Loss 0.9166925  Acc 0.6666666666666666  \n",
      "Epoch 446  Loss 0.91633993  Acc 0.6666666666666666  \n",
      "Epoch 447  Loss 0.9159919  Acc 0.6666666666666666  \n",
      "Epoch 448  Loss 0.9156389  Acc 0.6666666666666666  \n",
      "Epoch 449  Loss 0.91528493  Acc 0.6666666666666666  \n",
      "Epoch 450  Loss 0.91493946  Acc 0.6666666666666666  \n",
      "Epoch 451  Loss 0.9145865  Acc 0.6666666666666666  \n",
      "Epoch 452  Loss 0.9142416  Acc 0.6666666666666666  \n",
      "Epoch 453  Loss 0.91389364  Acc 0.6666666666666666  \n",
      "Epoch 454  Loss 0.91355175  Acc 0.6666666666666666  \n",
      "Epoch 455  Loss 0.91321015  Acc 0.6666666666666666  \n",
      "Epoch 456  Loss 0.9128614  Acc 0.6666666666666666  \n",
      "Epoch 457  Loss 0.91250294  Acc 0.6666666666666666  \n",
      "Epoch 458  Loss 0.91215533  Acc 0.6666666666666666  \n",
      "Epoch 459  Loss 0.91179365  Acc 0.6666666666666666  \n",
      "Epoch 460  Loss 0.91144717  Acc 0.6666666666666666  \n",
      "Epoch 461  Loss 0.91110593  Acc 0.6666666666666666  \n",
      "Epoch 462  Loss 0.9107604  Acc 0.6666666666666666  \n",
      "Epoch 463  Loss 0.91041136  Acc 0.6666666666666666  \n",
      "Epoch 464  Loss 0.91006  Acc 0.6666666666666666  \n",
      "Epoch 465  Loss 0.90970904  Acc 0.6666666666666666  \n",
      "Epoch 466  Loss 0.9093638  Acc 0.6666666666666666  \n",
      "Epoch 467  Loss 0.9090192  Acc 0.6666666666666666  \n",
      "Epoch 468  Loss 0.9086645  Acc 0.6666666666666666  \n",
      "Epoch 469  Loss 0.90831697  Acc 0.6666666666666666  \n",
      "Epoch 470  Loss 0.9079654  Acc 0.6666666666666666  \n",
      "Epoch 471  Loss 0.90761507  Acc 0.6666666666666666  \n",
      "Epoch 472  Loss 0.90725666  Acc 0.6666666666666666  \n",
      "Epoch 473  Loss 0.9069031  Acc 0.6666666666666666  \n",
      "Epoch 474  Loss 0.90655375  Acc 0.6666666666666666  \n",
      "Epoch 475  Loss 0.9062027  Acc 0.6666666666666666  \n",
      "Epoch 476  Loss 0.90586203  Acc 0.6666666666666666  \n",
      "Epoch 477  Loss 0.9055133  Acc 0.6666666666666666  \n",
      "Epoch 478  Loss 0.9051574  Acc 0.6666666666666666  \n",
      "Epoch 479  Loss 0.9048197  Acc 0.6666666666666666  \n",
      "Epoch 480  Loss 0.9044695  Acc 0.6666666666666666  \n",
      "Epoch 481  Loss 0.90411985  Acc 0.6666666666666666  \n",
      "Epoch 482  Loss 0.9037769  Acc 0.6666666666666666  \n",
      "Epoch 483  Loss 0.9034346  Acc 0.6666666666666666  \n",
      "Epoch 484  Loss 0.9030834  Acc 0.6666666666666666  \n",
      "Epoch 485  Loss 0.9027337  Acc 0.6666666666666666  \n",
      "Epoch 486  Loss 0.9023838  Acc 0.6666666666666666  \n",
      "Epoch 487  Loss 0.90203196  Acc 0.6666666666666666  \n",
      "Epoch 488  Loss 0.90168315  Acc 0.6666666666666666  \n",
      "Epoch 489  Loss 0.9013322  Acc 0.6666666666666666  \n",
      "Epoch 490  Loss 0.9009854  Acc 0.6666666666666666  \n",
      "Epoch 491  Loss 0.900632  Acc 0.6666666666666666  \n",
      "Epoch 492  Loss 0.9002752  Acc 0.6666666666666666  \n",
      "Epoch 493  Loss 0.89991343  Acc 0.6666666666666666  \n",
      "Epoch 494  Loss 0.8995695  Acc 0.6666666666666666  \n",
      "Epoch 495  Loss 0.89922523  Acc 0.6666666666666666  \n",
      "Epoch 496  Loss 0.8988866  Acc 0.6666666666666666  \n",
      "Epoch 497  Loss 0.89853525  Acc 0.6666666666666666  \n",
      "Epoch 498  Loss 0.89819217  Acc 0.6666666666666666  \n",
      "Epoch 499  Loss 0.8978417  Acc 0.6666666666666666  \n",
      "Epoch 500  Loss 0.89749044  Acc 0.6666666666666666  \n",
      "Epoch 501  Loss 0.89714867  Acc 0.6666666666666666  \n",
      "Epoch 502  Loss 0.896805  Acc 0.6666666666666666  \n",
      "Epoch 503  Loss 0.8964608  Acc 0.6666666666666666  \n",
      "Epoch 504  Loss 0.8961248  Acc 0.6666666666666666  \n",
      "Epoch 505  Loss 0.89577067  Acc 0.6666666666666666  \n",
      "Epoch 506  Loss 0.895422  Acc 0.6666666666666666  \n",
      "Epoch 507  Loss 0.8950775  Acc 0.6666666666666666  \n",
      "Epoch 508  Loss 0.8947231  Acc 0.6666666666666666  \n",
      "Epoch 509  Loss 0.8943722  Acc 0.6666666666666666  \n",
      "Epoch 510  Loss 0.89403063  Acc 0.6666666666666666  \n",
      "Epoch 511  Loss 0.89368796  Acc 0.6666666666666666  \n",
      "Epoch 512  Loss 0.89332944  Acc 0.6666666666666666  \n",
      "Epoch 513  Loss 0.89296925  Acc 0.6666666666666666  \n",
      "Epoch 514  Loss 0.8926272  Acc 0.6666666666666666  \n",
      "Epoch 515  Loss 0.8922731  Acc 0.6666666666666666  \n",
      "Epoch 516  Loss 0.89191943  Acc 0.6666666666666666  \n",
      "Epoch 517  Loss 0.89156914  Acc 0.6666666666666666  \n",
      "Epoch 518  Loss 0.8912211  Acc 0.6666666666666666  \n",
      "Epoch 519  Loss 0.8908797  Acc 0.6666666666666666  \n",
      "Epoch 520  Loss 0.8905385  Acc 0.6666666666666666  \n",
      "Epoch 521  Loss 0.8901979  Acc 0.6666666666666666  \n",
      "Epoch 522  Loss 0.8898459  Acc 0.6666666666666666  \n",
      "Epoch 523  Loss 0.88948935  Acc 0.6666666666666666  \n",
      "Epoch 524  Loss 0.889142  Acc 0.6666666666666666  \n",
      "Epoch 525  Loss 0.8887945  Acc 0.6666666666666666  \n",
      "Epoch 526  Loss 0.8884492  Acc 0.6666666666666666  \n",
      "Epoch 527  Loss 0.8881023  Acc 0.6666666666666666  \n",
      "Epoch 528  Loss 0.88775605  Acc 0.6666666666666666  \n",
      "Epoch 529  Loss 0.88739717  Acc 0.6666666666666666  \n",
      "Epoch 530  Loss 0.8870573  Acc 0.6666666666666666  \n",
      "Epoch 531  Loss 0.8867033  Acc 0.6666666666666666  \n",
      "Epoch 532  Loss 0.8863503  Acc 0.6666666666666666  \n",
      "Epoch 533  Loss 0.8860098  Acc 0.6666666666666666  \n",
      "Epoch 534  Loss 0.8856602  Acc 0.6666666666666666  \n",
      "Epoch 535  Loss 0.8853185  Acc 0.6666666666666666  \n",
      "Epoch 536  Loss 0.88497245  Acc 0.6666666666666666  \n",
      "Epoch 537  Loss 0.8846267  Acc 0.6666666666666666  \n",
      "Epoch 538  Loss 0.8842758  Acc 0.6666666666666666  \n",
      "Epoch 539  Loss 0.88394105  Acc 0.6666666666666666  \n",
      "Epoch 540  Loss 0.8835836  Acc 0.6666666666666666  \n",
      "Epoch 541  Loss 0.8832451  Acc 0.6666666666666666  \n",
      "Epoch 542  Loss 0.88288474  Acc 0.6666666666666666  \n",
      "Epoch 543  Loss 0.8825334  Acc 0.6666666666666666  \n",
      "Epoch 544  Loss 0.8821945  Acc 0.6666666666666666  \n",
      "Epoch 545  Loss 0.8818506  Acc 0.6666666666666666  \n",
      "Epoch 546  Loss 0.8815086  Acc 0.6666666666666666  \n",
      "Epoch 547  Loss 0.8811625  Acc 0.6666666666666666  \n",
      "Epoch 548  Loss 0.88083005  Acc 0.6666666666666666  \n",
      "Epoch 549  Loss 0.8804777  Acc 0.6666666666666666  \n",
      "Epoch 550  Loss 0.88012826  Acc 0.6666666666666666  \n",
      "Epoch 551  Loss 0.8797799  Acc 0.6666666666666666  \n",
      "Epoch 552  Loss 0.8794448  Acc 0.6666666666666666  \n",
      "Epoch 553  Loss 0.87908345  Acc 0.6666666666666666  \n",
      "Epoch 554  Loss 0.8787296  Acc 0.6666666666666666  \n",
      "Epoch 555  Loss 0.8783769  Acc 0.6666666666666666  \n",
      "Epoch 556  Loss 0.87802756  Acc 0.6666666666666666  \n",
      "Epoch 557  Loss 0.87767863  Acc 0.6666666666666666  \n",
      "Epoch 558  Loss 0.8773258  Acc 0.6666666666666666  \n",
      "Epoch 559  Loss 0.87698346  Acc 0.6666666666666666  \n",
      "Epoch 560  Loss 0.87664145  Acc 0.6666666666666666  \n",
      "Epoch 561  Loss 0.8762976  Acc 0.6666666666666666  \n",
      "Epoch 562  Loss 0.87595814  Acc 0.6666666666666666  \n",
      "Epoch 563  Loss 0.8756147  Acc 0.6666666666666666  \n",
      "Epoch 564  Loss 0.87527984  Acc 0.6666666666666666  \n",
      "Epoch 565  Loss 0.8749336  Acc 0.6666666666666666  \n",
      "Epoch 566  Loss 0.8745938  Acc 0.6666666666666666  \n",
      "Epoch 567  Loss 0.8742558  Acc 0.6666666666666666  \n",
      "Epoch 568  Loss 0.8739089  Acc 0.6666666666666666  \n",
      "Epoch 569  Loss 0.87356395  Acc 0.6666666666666666  \n",
      "Epoch 570  Loss 0.87321675  Acc 0.6666666666666666  \n",
      "Epoch 571  Loss 0.872875  Acc 0.6666666666666666  \n",
      "Epoch 572  Loss 0.8725348  Acc 0.6666666666666666  \n",
      "Epoch 573  Loss 0.87219006  Acc 0.6666666666666666  \n",
      "Epoch 574  Loss 0.8718508  Acc 0.6666666666666666  \n",
      "Epoch 575  Loss 0.87151325  Acc 0.6666666666666666  \n",
      "Epoch 576  Loss 0.8711696  Acc 0.6666666666666666  \n",
      "Epoch 577  Loss 0.8708313  Acc 0.6666666666666666  \n",
      "Epoch 578  Loss 0.87048036  Acc 0.6666666666666666  \n",
      "Epoch 579  Loss 0.8701412  Acc 0.6666666666666666  \n",
      "Epoch 580  Loss 0.86980873  Acc 0.6666666666666666  \n",
      "Epoch 581  Loss 0.8694579  Acc 0.6666666666666666  \n",
      "Epoch 582  Loss 0.86911446  Acc 0.6666666666666666  \n",
      "Epoch 583  Loss 0.86876345  Acc 0.6666666666666666  \n",
      "Epoch 584  Loss 0.86842245  Acc 0.6666666666666666  \n",
      "Epoch 585  Loss 0.8680843  Acc 0.6666666666666666  \n",
      "Epoch 586  Loss 0.8677332  Acc 0.6666666666666666  \n",
      "Epoch 587  Loss 0.8673923  Acc 0.6666666666666666  \n",
      "Epoch 588  Loss 0.86705244  Acc 0.6666666666666666  \n",
      "Epoch 589  Loss 0.8667164  Acc 0.6666666666666666  \n",
      "Epoch 590  Loss 0.86637837  Acc 0.6666666666666666  \n",
      "Epoch 591  Loss 0.86603904  Acc 0.6666666666666666  \n",
      "Epoch 592  Loss 0.8657071  Acc 0.6666666666666666  \n",
      "Epoch 593  Loss 0.8653707  Acc 0.6666666666666666  \n",
      "Epoch 594  Loss 0.8650287  Acc 0.6666666666666666  \n",
      "Epoch 595  Loss 0.86468446  Acc 0.6666666666666666  \n",
      "Epoch 596  Loss 0.8643436  Acc 0.6666666666666666  \n",
      "Epoch 597  Loss 0.86400175  Acc 0.6666666666666666  \n",
      "Epoch 598  Loss 0.8636686  Acc 0.6666666666666666  \n",
      "Epoch 599  Loss 0.8633335  Acc 0.6666666666666666  \n",
      "Epoch 600  Loss 0.8629885  Acc 0.6666666666666666  \n",
      "Epoch 601  Loss 0.8626597  Acc 0.6666666666666666  \n",
      "Epoch 602  Loss 0.86230266  Acc 0.6666666666666666  \n",
      "Epoch 603  Loss 0.8619703  Acc 0.6666666666666666  \n",
      "Epoch 604  Loss 0.8616385  Acc 0.6666666666666666  \n",
      "Epoch 605  Loss 0.86129373  Acc 0.6666666666666666  \n",
      "Epoch 606  Loss 0.8609472  Acc 0.6666666666666666  \n",
      "Epoch 607  Loss 0.86061704  Acc 0.6666666666666666  \n",
      "Epoch 608  Loss 0.8602852  Acc 0.6666666666666666  \n",
      "Epoch 609  Loss 0.85994077  Acc 0.6666666666666666  \n",
      "Epoch 610  Loss 0.85960674  Acc 0.6666666666666666  \n",
      "Epoch 611  Loss 0.8592715  Acc 0.6666666666666666  \n",
      "Epoch 612  Loss 0.8589368  Acc 0.6666666666666666  \n",
      "Epoch 613  Loss 0.8585893  Acc 0.6666666666666666  \n",
      "Epoch 614  Loss 0.858255  Acc 0.6666666666666666  \n",
      "Epoch 615  Loss 0.8579051  Acc 0.6666666666666666  \n",
      "Epoch 616  Loss 0.857566  Acc 0.6666666666666666  \n",
      "Epoch 617  Loss 0.85723245  Acc 0.6666666666666666  \n",
      "Epoch 618  Loss 0.8568887  Acc 0.6666666666666666  \n",
      "Epoch 619  Loss 0.85654265  Acc 0.6666666666666666  \n",
      "Epoch 620  Loss 0.85620373  Acc 0.6666666666666666  \n",
      "Epoch 621  Loss 0.8558573  Acc 0.6666666666666666  \n",
      "Epoch 622  Loss 0.85551983  Acc 0.6666666666666666  \n",
      "Epoch 623  Loss 0.85518175  Acc 0.6666666666666666  \n",
      "Epoch 624  Loss 0.8548463  Acc 0.6666666666666666  \n",
      "Epoch 625  Loss 0.85451263  Acc 0.6666666666666666  \n",
      "Epoch 626  Loss 0.8541696  Acc 0.6666666666666666  \n",
      "Epoch 627  Loss 0.85382557  Acc 0.6666666666666666  \n",
      "Epoch 628  Loss 0.8534847  Acc 0.6666666666666666  \n",
      "Epoch 629  Loss 0.8531443  Acc 0.6666666666666666  \n",
      "Epoch 630  Loss 0.85281014  Acc 0.6666666666666666  \n",
      "Epoch 631  Loss 0.85247356  Acc 0.6666666666666666  \n",
      "Epoch 632  Loss 0.852137  Acc 0.6666666666666666  \n",
      "Epoch 633  Loss 0.8517972  Acc 0.6666666666666666  \n",
      "Epoch 634  Loss 0.8514505  Acc 0.6666666666666666  \n",
      "Epoch 635  Loss 0.8511091  Acc 0.6666666666666666  \n",
      "Epoch 636  Loss 0.8507805  Acc 0.6666666666666666  \n",
      "Epoch 637  Loss 0.85044676  Acc 0.6666666666666666  \n",
      "Epoch 638  Loss 0.85010636  Acc 0.6666666666666666  \n",
      "Epoch 639  Loss 0.8497701  Acc 0.6666666666666666  \n",
      "Epoch 640  Loss 0.8494342  Acc 0.6666666666666666  \n",
      "Epoch 641  Loss 0.8490838  Acc 0.6666666666666666  \n",
      "Epoch 642  Loss 0.84874874  Acc 0.6666666666666666  \n",
      "Epoch 643  Loss 0.84840435  Acc 0.6666666666666666  \n",
      "Epoch 644  Loss 0.8480667  Acc 0.6666666666666666  \n",
      "Epoch 645  Loss 0.8477309  Acc 0.6666666666666666  \n",
      "Epoch 646  Loss 0.84739524  Acc 0.6666666666666666  \n",
      "Epoch 647  Loss 0.8470589  Acc 0.6666666666666666  \n",
      "Epoch 648  Loss 0.8467298  Acc 0.6666666666666666  \n",
      "Epoch 649  Loss 0.84638685  Acc 0.6666666666666666  \n",
      "Epoch 650  Loss 0.846057  Acc 0.6666666666666666  \n",
      "Epoch 651  Loss 0.84572244  Acc 0.6666666666666666  \n",
      "Epoch 652  Loss 0.8453787  Acc 0.6666666666666666  \n",
      "Epoch 653  Loss 0.8450324  Acc 0.6666666666666666  \n",
      "Epoch 654  Loss 0.8447015  Acc 0.6666666666666666  \n",
      "Epoch 655  Loss 0.84436435  Acc 0.6666666666666666  \n",
      "Epoch 656  Loss 0.84402925  Acc 0.6666666666666666  \n",
      "Epoch 657  Loss 0.84368527  Acc 0.6666666666666666  \n",
      "Epoch 658  Loss 0.8433485  Acc 0.6666666666666666  \n",
      "Epoch 659  Loss 0.8430078  Acc 0.6666666666666666  \n",
      "Epoch 660  Loss 0.8426597  Acc 0.6666666666666666  \n",
      "Epoch 661  Loss 0.842335  Acc 0.6666666666666666  \n",
      "Epoch 662  Loss 0.8420055  Acc 0.6666666666666666  \n",
      "Epoch 663  Loss 0.8416762  Acc 0.6666666666666666  \n",
      "Epoch 664  Loss 0.84134483  Acc 0.6666666666666666  \n",
      "Epoch 665  Loss 0.84101194  Acc 0.6666666666666666  \n",
      "Epoch 666  Loss 0.8406699  Acc 0.6666666666666666  \n",
      "Epoch 667  Loss 0.8403235  Acc 0.6666666666666666  \n",
      "Epoch 668  Loss 0.8399938  Acc 0.6666666666666666  \n",
      "Epoch 669  Loss 0.83964425  Acc 0.6666666666666666  \n",
      "Epoch 670  Loss 0.83930445  Acc 0.6666666666666666  \n",
      "Epoch 671  Loss 0.8389616  Acc 0.6666666666666666  \n",
      "Epoch 672  Loss 0.838637  Acc 0.6666666666666666  \n",
      "Epoch 673  Loss 0.8383081  Acc 0.6666666666666666  \n",
      "Epoch 674  Loss 0.83798164  Acc 0.6666666666666666  \n",
      "Epoch 675  Loss 0.8376456  Acc 0.6666666666666666  \n",
      "Epoch 676  Loss 0.8373094  Acc 0.6666666666666666  \n",
      "Epoch 677  Loss 0.83697736  Acc 0.6666666666666666  \n",
      "Epoch 678  Loss 0.8366361  Acc 0.6666666666666666  \n",
      "Epoch 679  Loss 0.836309  Acc 0.6666666666666666  \n",
      "Epoch 680  Loss 0.83597577  Acc 0.6666666666666666  \n",
      "Epoch 681  Loss 0.8356392  Acc 0.6666666666666666  \n",
      "Epoch 682  Loss 0.8353001  Acc 0.6666666666666666  \n",
      "Epoch 683  Loss 0.8349673  Acc 0.6666666666666666  \n",
      "Epoch 684  Loss 0.8346241  Acc 0.6666666666666666  \n",
      "Epoch 685  Loss 0.8342868  Acc 0.6666666666666666  \n",
      "Epoch 686  Loss 0.83396494  Acc 0.6666666666666666  \n",
      "Epoch 687  Loss 0.8336322  Acc 0.6666666666666666  \n",
      "Epoch 688  Loss 0.8332986  Acc 0.6666666666666666  \n",
      "Epoch 689  Loss 0.8329708  Acc 0.6666666666666666  \n",
      "Epoch 690  Loss 0.8326234  Acc 0.6666666666666666  \n",
      "Epoch 691  Loss 0.8322901  Acc 0.6666666666666666  \n",
      "Epoch 692  Loss 0.8319527  Acc 0.6666666666666666  \n",
      "Epoch 693  Loss 0.8316111  Acc 0.6666666666666666  \n",
      "Epoch 694  Loss 0.83127445  Acc 0.6666666666666666  \n",
      "Epoch 695  Loss 0.8309514  Acc 0.6666666666666666  \n",
      "Epoch 696  Loss 0.8306074  Acc 0.6666666666666666  \n",
      "Epoch 697  Loss 0.8302731  Acc 0.6666666666666666  \n",
      "Epoch 698  Loss 0.82993746  Acc 0.6666666666666666  \n",
      "Epoch 699  Loss 0.8296138  Acc 0.6666666666666666  \n",
      "Epoch 700  Loss 0.8292806  Acc 0.6666666666666666  \n",
      "Epoch 701  Loss 0.8289502  Acc 0.6666666666666666  \n",
      "Epoch 702  Loss 0.82861036  Acc 0.6666666666666666  \n",
      "Epoch 703  Loss 0.82827514  Acc 0.6666666666666666  \n",
      "Epoch 704  Loss 0.8279393  Acc 0.6666666666666666  \n",
      "Epoch 705  Loss 0.8276031  Acc 0.6666666666666666  \n",
      "Epoch 706  Loss 0.82727  Acc 0.6666666666666666  \n",
      "Epoch 707  Loss 0.82694054  Acc 0.6666666666666666  \n",
      "Epoch 708  Loss 0.82661146  Acc 0.6666666666666666  \n",
      "Epoch 709  Loss 0.82628393  Acc 0.6666666666666666  \n",
      "Epoch 710  Loss 0.8259561  Acc 0.6666666666666666  \n",
      "Epoch 711  Loss 0.82562536  Acc 0.6666666666666666  \n",
      "Epoch 712  Loss 0.8252937  Acc 0.6666666666666666  \n",
      "Epoch 713  Loss 0.8249555  Acc 0.6666666666666666  \n",
      "Epoch 714  Loss 0.8246288  Acc 0.6666666666666666  \n",
      "Epoch 715  Loss 0.82429475  Acc 0.6666666666666666  \n",
      "Epoch 716  Loss 0.82396054  Acc 0.6666666666666666  \n",
      "Epoch 717  Loss 0.8236186  Acc 0.6666666666666666  \n",
      "Epoch 718  Loss 0.82328707  Acc 0.6666666666666666  \n",
      "Epoch 719  Loss 0.8229579  Acc 0.6666666666666666  \n",
      "Epoch 720  Loss 0.8226307  Acc 0.6666666666666666  \n",
      "Epoch 721  Loss 0.8222954  Acc 0.6666666666666666  \n",
      "Epoch 722  Loss 0.82196474  Acc 0.6666666666666666  \n",
      "Epoch 723  Loss 0.821635  Acc 0.6666666666666666  \n",
      "Epoch 724  Loss 0.82130486  Acc 0.6666666666666666  \n",
      "Epoch 725  Loss 0.8209743  Acc 0.6666666666666666  \n",
      "Epoch 726  Loss 0.82065254  Acc 0.6666666666666666  \n",
      "Epoch 727  Loss 0.82032883  Acc 0.6666666666666666  \n",
      "Epoch 728  Loss 0.8199974  Acc 0.6666666666666666  \n",
      "Epoch 729  Loss 0.81966877  Acc 0.6666666666666666  \n",
      "Epoch 730  Loss 0.81933725  Acc 0.6666666666666666  \n",
      "Epoch 731  Loss 0.81900615  Acc 0.6666666666666666  \n",
      "Epoch 732  Loss 0.81868404  Acc 0.6666666666666666  \n",
      "Epoch 733  Loss 0.81834996  Acc 0.6666666666666666  \n",
      "Epoch 734  Loss 0.8180225  Acc 0.6666666666666666  \n",
      "Epoch 735  Loss 0.8176944  Acc 0.6666666666666666  \n",
      "Epoch 736  Loss 0.8173672  Acc 0.6666666666666666  \n",
      "Epoch 737  Loss 0.81704164  Acc 0.6666666666666666  \n",
      "Epoch 738  Loss 0.8167128  Acc 0.6666666666666666  \n",
      "Epoch 739  Loss 0.81637925  Acc 0.6666666666666666  \n",
      "Epoch 740  Loss 0.8160562  Acc 0.6666666666666666  \n",
      "Epoch 741  Loss 0.81572545  Acc 0.6666666666666666  \n",
      "Epoch 742  Loss 0.81539994  Acc 0.6666666666666666  \n",
      "Epoch 743  Loss 0.8150712  Acc 0.6666666666666666  \n",
      "Epoch 744  Loss 0.8147356  Acc 0.6666666666666666  \n",
      "Epoch 745  Loss 0.81441224  Acc 0.6666666666666666  \n",
      "Epoch 746  Loss 0.8140902  Acc 0.6666666666666666  \n",
      "Epoch 747  Loss 0.81376696  Acc 0.6666666666666666  \n",
      "Epoch 748  Loss 0.8134381  Acc 0.6666666666666666  \n",
      "Epoch 749  Loss 0.81310695  Acc 0.6666666666666666  \n",
      "Epoch 750  Loss 0.8127741  Acc 0.6666666666666666  \n",
      "Epoch 751  Loss 0.81244963  Acc 0.6666666666666666  \n",
      "Epoch 752  Loss 0.8121299  Acc 0.6666666666666666  \n",
      "Epoch 753  Loss 0.8117983  Acc 0.6666666666666666  \n",
      "Epoch 754  Loss 0.81146896  Acc 0.6666666666666666  \n",
      "Epoch 755  Loss 0.8111515  Acc 0.6666666666666666  \n",
      "Epoch 756  Loss 0.81081945  Acc 0.6666666666666666  \n",
      "Epoch 757  Loss 0.8104962  Acc 0.6666666666666666  \n",
      "Epoch 758  Loss 0.8101667  Acc 0.6666666666666666  \n",
      "Epoch 759  Loss 0.80984026  Acc 0.6666666666666666  \n",
      "Epoch 760  Loss 0.8095113  Acc 0.6666666666666666  \n",
      "Epoch 761  Loss 0.80918247  Acc 0.6666666666666666  \n",
      "Epoch 762  Loss 0.808858  Acc 0.6666666666666666  \n",
      "Epoch 763  Loss 0.80853343  Acc 0.6666666666666666  \n",
      "Epoch 764  Loss 0.8082032  Acc 0.6666666666666666  \n",
      "Epoch 765  Loss 0.8078726  Acc 0.6666666666666666  \n",
      "Epoch 766  Loss 0.80754393  Acc 0.6666666666666666  \n",
      "Epoch 767  Loss 0.80721456  Acc 0.6666666666666666  \n",
      "Epoch 768  Loss 0.806893  Acc 0.6666666666666666  \n",
      "Epoch 769  Loss 0.8065697  Acc 0.6666666666666666  \n",
      "Epoch 770  Loss 0.80624676  Acc 0.6666666666666666  \n",
      "Epoch 771  Loss 0.8059235  Acc 0.6666666666666666  \n",
      "Epoch 772  Loss 0.8056021  Acc 0.6666666666666666  \n",
      "Epoch 773  Loss 0.8052715  Acc 0.6666666666666666  \n",
      "Epoch 774  Loss 0.8049505  Acc 0.6666666666666666  \n",
      "Epoch 775  Loss 0.8046256  Acc 0.6666666666666666  \n",
      "Epoch 776  Loss 0.80429757  Acc 0.6666666666666666  \n",
      "Epoch 777  Loss 0.80397457  Acc 0.6666666666666666  \n",
      "Epoch 778  Loss 0.8036552  Acc 0.6666666666666666  \n",
      "Epoch 779  Loss 0.8033258  Acc 0.6666666666666666  \n",
      "Epoch 780  Loss 0.8030036  Acc 0.6666666666666666  \n",
      "Epoch 781  Loss 0.8026872  Acc 0.6666666666666666  \n",
      "Epoch 782  Loss 0.8023642  Acc 0.6666666666666666  \n",
      "Epoch 783  Loss 0.8020458  Acc 0.6666666666666666  \n",
      "Epoch 784  Loss 0.8017189  Acc 0.6666666666666666  \n",
      "Epoch 785  Loss 0.8013957  Acc 0.6666666666666666  \n",
      "Epoch 786  Loss 0.8010766  Acc 0.6666666666666666  \n",
      "Epoch 787  Loss 0.8007426  Acc 0.6666666666666666  \n",
      "Epoch 788  Loss 0.8004204  Acc 0.6666666666666666  \n",
      "Epoch 789  Loss 0.8000899  Acc 0.6666666666666666  \n",
      "Epoch 790  Loss 0.799771  Acc 0.6666666666666666  \n",
      "Epoch 791  Loss 0.79944474  Acc 0.6666666666666666  \n",
      "Epoch 792  Loss 0.79912794  Acc 0.6666666666666666  \n",
      "Epoch 793  Loss 0.79880553  Acc 0.6666666666666666  \n",
      "Epoch 794  Loss 0.79848284  Acc 0.6666666666666666  \n",
      "Epoch 795  Loss 0.7981573  Acc 0.6666666666666666  \n",
      "Epoch 796  Loss 0.7978302  Acc 0.6666666666666666  \n",
      "Epoch 797  Loss 0.797514  Acc 0.6666666666666666  \n",
      "Epoch 798  Loss 0.7971929  Acc 0.6666666666666666  \n",
      "Epoch 799  Loss 0.7968672  Acc 0.6666666666666666  \n",
      "Epoch 800  Loss 0.7965419  Acc 0.6666666666666666  \n",
      "Epoch 801  Loss 0.7962173  Acc 0.6666666666666666  \n",
      "Epoch 802  Loss 0.7958911  Acc 0.6666666666666666  \n",
      "Epoch 803  Loss 0.7955749  Acc 0.6666666666666666  \n",
      "Epoch 804  Loss 0.79524523  Acc 0.6666666666666666  \n",
      "Epoch 805  Loss 0.79492897  Acc 0.6666666666666666  \n",
      "Epoch 806  Loss 0.7946072  Acc 0.6666666666666666  \n",
      "Epoch 807  Loss 0.79428214  Acc 0.6666666666666666  \n",
      "Epoch 808  Loss 0.79396033  Acc 0.6666666666666666  \n",
      "Epoch 809  Loss 0.7936407  Acc 0.6666666666666666  \n",
      "Epoch 810  Loss 0.7933209  Acc 0.6666666666666666  \n",
      "Epoch 811  Loss 0.79300475  Acc 0.6666666666666666  \n",
      "Epoch 812  Loss 0.79268914  Acc 0.6666666666666666  \n",
      "Epoch 813  Loss 0.7923741  Acc 0.6666666666666666  \n",
      "Epoch 814  Loss 0.7920635  Acc 0.6666666666666666  \n",
      "Epoch 815  Loss 0.7917475  Acc 0.6666666666666666  \n",
      "Epoch 816  Loss 0.79143167  Acc 0.6666666666666666  \n",
      "Epoch 817  Loss 0.7911167  Acc 0.6666666666666666  \n",
      "Epoch 818  Loss 0.790792  Acc 0.6666666666666666  \n",
      "Epoch 819  Loss 0.7904663  Acc 0.6666666666666666  \n",
      "Epoch 820  Loss 0.79014295  Acc 0.6666666666666666  \n",
      "Epoch 821  Loss 0.7898245  Acc 0.6666666666666666  \n",
      "Epoch 822  Loss 0.78950655  Acc 0.6666666666666666  \n",
      "Epoch 823  Loss 0.7891836  Acc 0.6666666666666666  \n",
      "Epoch 824  Loss 0.78886616  Acc 0.6733333333333333  \n",
      "Epoch 825  Loss 0.78854394  Acc 0.6666666666666666  \n",
      "Epoch 826  Loss 0.7882315  Acc 0.6733333333333333  \n",
      "Epoch 827  Loss 0.78790957  Acc 0.6733333333333333  \n",
      "Epoch 828  Loss 0.7875975  Acc 0.6733333333333333  \n",
      "Epoch 829  Loss 0.78728133  Acc 0.6733333333333333  \n",
      "Epoch 830  Loss 0.7869683  Acc 0.6733333333333333  \n",
      "Epoch 831  Loss 0.7866588  Acc 0.6733333333333333  \n",
      "Epoch 832  Loss 0.7863428  Acc 0.6733333333333333  \n",
      "Epoch 833  Loss 0.7860265  Acc 0.6733333333333333  \n",
      "Epoch 834  Loss 0.7857141  Acc 0.6733333333333333  \n",
      "Epoch 835  Loss 0.7853966  Acc 0.6733333333333333  \n",
      "Epoch 836  Loss 0.7850836  Acc 0.6733333333333333  \n",
      "Epoch 837  Loss 0.7847708  Acc 0.6733333333333333  \n",
      "Epoch 838  Loss 0.7844547  Acc 0.6733333333333333  \n",
      "Epoch 839  Loss 0.784133  Acc 0.6733333333333333  \n",
      "Epoch 840  Loss 0.7838164  Acc 0.6733333333333333  \n",
      "Epoch 841  Loss 0.7834974  Acc 0.6733333333333333  \n",
      "Epoch 842  Loss 0.78318113  Acc 0.6733333333333333  \n",
      "Epoch 843  Loss 0.7828654  Acc 0.6733333333333333  \n",
      "Epoch 844  Loss 0.7825505  Acc 0.6733333333333333  \n",
      "Epoch 845  Loss 0.7822354  Acc 0.6733333333333333  \n",
      "Epoch 846  Loss 0.7819243  Acc 0.6733333333333333  \n",
      "Epoch 847  Loss 0.7816139  Acc 0.6733333333333333  \n",
      "Epoch 848  Loss 0.7812954  Acc 0.6733333333333333  \n",
      "Epoch 849  Loss 0.7809748  Acc 0.6733333333333333  \n",
      "Epoch 850  Loss 0.7806559  Acc 0.6733333333333333  \n",
      "Epoch 851  Loss 0.7803454  Acc 0.6733333333333333  \n",
      "Epoch 852  Loss 0.7800376  Acc 0.6733333333333333  \n",
      "Epoch 853  Loss 0.7797326  Acc 0.6733333333333333  \n",
      "Epoch 854  Loss 0.77942294  Acc 0.6733333333333333  \n",
      "Epoch 855  Loss 0.7791053  Acc 0.6733333333333333  \n",
      "Epoch 856  Loss 0.7787896  Acc 0.6733333333333333  \n",
      "Epoch 857  Loss 0.77847475  Acc 0.6733333333333333  \n",
      "Epoch 858  Loss 0.77817106  Acc 0.6733333333333333  \n",
      "Epoch 859  Loss 0.77785194  Acc 0.6733333333333333  \n",
      "Epoch 860  Loss 0.7775366  Acc 0.6733333333333333  \n",
      "Epoch 861  Loss 0.77722335  Acc 0.6733333333333333  \n",
      "Epoch 862  Loss 0.7769147  Acc 0.6733333333333333  \n",
      "Epoch 863  Loss 0.77660114  Acc 0.6733333333333333  \n",
      "Epoch 864  Loss 0.77628726  Acc 0.6733333333333333  \n",
      "Epoch 865  Loss 0.77597445  Acc 0.6733333333333333  \n",
      "Epoch 866  Loss 0.775666  Acc 0.6733333333333333  \n",
      "Epoch 867  Loss 0.775358  Acc 0.6733333333333333  \n",
      "Epoch 868  Loss 0.775041  Acc 0.6733333333333333  \n",
      "Epoch 869  Loss 0.7747235  Acc 0.6733333333333333  \n",
      "Epoch 870  Loss 0.77441144  Acc 0.6733333333333333  \n",
      "Epoch 871  Loss 0.7741038  Acc 0.6733333333333333  \n",
      "Epoch 872  Loss 0.7737882  Acc 0.6733333333333333  \n",
      "Epoch 873  Loss 0.77347225  Acc 0.6733333333333333  \n",
      "Epoch 874  Loss 0.7731549  Acc 0.6733333333333333  \n",
      "Epoch 875  Loss 0.7728351  Acc 0.6733333333333333  \n",
      "Epoch 876  Loss 0.77252424  Acc 0.6733333333333333  \n",
      "Epoch 877  Loss 0.7722175  Acc 0.6733333333333333  \n",
      "Epoch 878  Loss 0.7719028  Acc 0.6733333333333333  \n",
      "Epoch 879  Loss 0.77159315  Acc 0.6733333333333333  \n",
      "Epoch 880  Loss 0.7712671  Acc 0.6733333333333333  \n",
      "Epoch 881  Loss 0.7709513  Acc 0.6733333333333333  \n",
      "Epoch 882  Loss 0.7706439  Acc 0.6733333333333333  \n",
      "Epoch 883  Loss 0.770338  Acc 0.6733333333333333  \n",
      "Epoch 884  Loss 0.7700245  Acc 0.6733333333333333  \n",
      "Epoch 885  Loss 0.76970994  Acc 0.6733333333333333  \n",
      "Epoch 886  Loss 0.7694003  Acc 0.6733333333333333  \n",
      "Epoch 887  Loss 0.76908726  Acc 0.6733333333333333  \n",
      "Epoch 888  Loss 0.76877207  Acc 0.6733333333333333  \n",
      "Epoch 889  Loss 0.76846385  Acc 0.6733333333333333  \n",
      "Epoch 890  Loss 0.76815474  Acc 0.6733333333333333  \n",
      "Epoch 891  Loss 0.7678469  Acc 0.6733333333333333  \n",
      "Epoch 892  Loss 0.76754194  Acc 0.6733333333333333  \n",
      "Epoch 893  Loss 0.7672341  Acc 0.6733333333333333  \n",
      "Epoch 894  Loss 0.766917  Acc 0.6733333333333333  \n",
      "Epoch 895  Loss 0.76660854  Acc 0.6733333333333333  \n",
      "Epoch 896  Loss 0.7663036  Acc 0.6733333333333333  \n",
      "Epoch 897  Loss 0.76598924  Acc 0.6733333333333333  \n",
      "Epoch 898  Loss 0.765686  Acc 0.6733333333333333  \n",
      "Epoch 899  Loss 0.76538104  Acc 0.6733333333333333  \n",
      "Epoch 900  Loss 0.7650717  Acc 0.6733333333333333  \n",
      "Epoch 901  Loss 0.7647679  Acc 0.6733333333333333  \n",
      "Epoch 902  Loss 0.76445127  Acc 0.6733333333333333  \n",
      "Epoch 903  Loss 0.7641428  Acc 0.6733333333333333  \n",
      "Epoch 904  Loss 0.76384026  Acc 0.6733333333333333  \n",
      "Epoch 905  Loss 0.76352984  Acc 0.6733333333333333  \n",
      "Epoch 906  Loss 0.7632269  Acc 0.6733333333333333  \n",
      "Epoch 907  Loss 0.7629116  Acc 0.6733333333333333  \n",
      "Epoch 908  Loss 0.7626016  Acc 0.6733333333333333  \n",
      "Epoch 909  Loss 0.7622954  Acc 0.6733333333333333  \n",
      "Epoch 910  Loss 0.761987  Acc 0.6733333333333333  \n",
      "Epoch 911  Loss 0.76167595  Acc 0.6733333333333333  \n",
      "Epoch 912  Loss 0.76137143  Acc 0.6733333333333333  \n",
      "Epoch 913  Loss 0.7610618  Acc 0.6733333333333333  \n",
      "Epoch 914  Loss 0.76076096  Acc 0.6733333333333333  \n",
      "Epoch 915  Loss 0.7604546  Acc 0.6733333333333333  \n",
      "Epoch 916  Loss 0.76014763  Acc 0.6733333333333333  \n",
      "Epoch 917  Loss 0.75983727  Acc 0.6733333333333333  \n",
      "Epoch 918  Loss 0.7595291  Acc 0.6733333333333333  \n",
      "Epoch 919  Loss 0.7592139  Acc 0.6733333333333333  \n",
      "Epoch 920  Loss 0.7589051  Acc 0.6733333333333333  \n",
      "Epoch 921  Loss 0.7586029  Acc 0.6733333333333333  \n",
      "Epoch 922  Loss 0.7583054  Acc 0.6733333333333333  \n",
      "Epoch 923  Loss 0.75800097  Acc 0.6733333333333333  \n",
      "Epoch 924  Loss 0.75769264  Acc 0.6733333333333333  \n",
      "Epoch 925  Loss 0.7573913  Acc 0.6733333333333333  \n",
      "Epoch 926  Loss 0.75709003  Acc 0.6733333333333333  \n",
      "Epoch 927  Loss 0.75679016  Acc 0.6733333333333333  \n",
      "Epoch 928  Loss 0.7564841  Acc 0.6733333333333333  \n",
      "Epoch 929  Loss 0.756181  Acc 0.6733333333333333  \n",
      "Epoch 930  Loss 0.7558775  Acc 0.6733333333333333  \n",
      "Epoch 931  Loss 0.7555713  Acc 0.6733333333333333  \n",
      "Epoch 932  Loss 0.75527006  Acc 0.6733333333333333  \n",
      "Epoch 933  Loss 0.7549668  Acc 0.6733333333333333  \n",
      "Epoch 934  Loss 0.75466084  Acc 0.6733333333333333  \n",
      "Epoch 935  Loss 0.75435805  Acc 0.6733333333333333  \n",
      "Epoch 936  Loss 0.7540508  Acc 0.6733333333333333  \n",
      "Epoch 937  Loss 0.7537429  Acc 0.68  \n",
      "Epoch 938  Loss 0.75344396  Acc 0.68  \n",
      "Epoch 939  Loss 0.7531421  Acc 0.68  \n",
      "Epoch 940  Loss 0.7528357  Acc 0.68  \n",
      "Epoch 941  Loss 0.752533  Acc 0.68  \n",
      "Epoch 942  Loss 0.7522311  Acc 0.68  \n",
      "Epoch 943  Loss 0.7519332  Acc 0.68  \n",
      "Epoch 944  Loss 0.7516294  Acc 0.68  \n",
      "Epoch 945  Loss 0.75133246  Acc 0.68  \n",
      "Epoch 946  Loss 0.7510319  Acc 0.68  \n",
      "Epoch 947  Loss 0.7507337  Acc 0.68  \n",
      "Epoch 948  Loss 0.75042886  Acc 0.68  \n",
      "Epoch 949  Loss 0.7501309  Acc 0.68  \n",
      "Epoch 950  Loss 0.74983484  Acc 0.68  \n",
      "Epoch 951  Loss 0.7495344  Acc 0.68  \n",
      "Epoch 952  Loss 0.7492382  Acc 0.68  \n",
      "Epoch 953  Loss 0.7489355  Acc 0.68  \n",
      "Epoch 954  Loss 0.7486377  Acc 0.68  \n",
      "Epoch 955  Loss 0.7483424  Acc 0.68  \n",
      "Epoch 956  Loss 0.74804205  Acc 0.68  \n",
      "Epoch 957  Loss 0.7477426  Acc 0.68  \n",
      "Epoch 958  Loss 0.7474434  Acc 0.68  \n",
      "Epoch 959  Loss 0.7471436  Acc 0.68  \n",
      "Epoch 960  Loss 0.74684024  Acc 0.68  \n",
      "Epoch 961  Loss 0.7465382  Acc 0.68  \n",
      "Epoch 962  Loss 0.746235  Acc 0.68  \n",
      "Epoch 963  Loss 0.7459383  Acc 0.68  \n",
      "Epoch 964  Loss 0.745639  Acc 0.68  \n",
      "Epoch 965  Loss 0.7453429  Acc 0.68  \n",
      "Epoch 966  Loss 0.745042  Acc 0.68  \n",
      "Epoch 967  Loss 0.744747  Acc 0.68  \n",
      "Epoch 968  Loss 0.74444884  Acc 0.68  \n",
      "Epoch 969  Loss 0.74415135  Acc 0.68  \n",
      "Epoch 970  Loss 0.7438548  Acc 0.68  \n",
      "Epoch 971  Loss 0.7435644  Acc 0.68  \n",
      "Epoch 972  Loss 0.74326754  Acc 0.68  \n",
      "Epoch 973  Loss 0.7429747  Acc 0.68  \n",
      "Epoch 974  Loss 0.74267554  Acc 0.68  \n",
      "Epoch 975  Loss 0.7423826  Acc 0.68  \n",
      "Epoch 976  Loss 0.74208367  Acc 0.68  \n",
      "Epoch 977  Loss 0.74178976  Acc 0.68  \n",
      "Epoch 978  Loss 0.74149454  Acc 0.68  \n",
      "Epoch 979  Loss 0.7412004  Acc 0.68  \n",
      "Epoch 980  Loss 0.7409058  Acc 0.68  \n",
      "Epoch 981  Loss 0.7406137  Acc 0.68  \n",
      "Epoch 982  Loss 0.7403112  Acc 0.68  \n",
      "Epoch 983  Loss 0.74001193  Acc 0.68  \n",
      "Epoch 984  Loss 0.73971164  Acc 0.68  \n",
      "Epoch 985  Loss 0.73942083  Acc 0.68  \n",
      "Epoch 986  Loss 0.7391256  Acc 0.68  \n",
      "Epoch 987  Loss 0.73882693  Acc 0.68  \n",
      "Epoch 988  Loss 0.73853594  Acc 0.68  \n",
      "Epoch 989  Loss 0.7382454  Acc 0.68  \n",
      "Epoch 990  Loss 0.7379428  Acc 0.68  \n",
      "Epoch 991  Loss 0.7376475  Acc 0.68  \n",
      "Epoch 992  Loss 0.7373432  Acc 0.68  \n",
      "Epoch 993  Loss 0.7370524  Acc 0.68  \n",
      "Epoch 994  Loss 0.736759  Acc 0.68  \n",
      "Epoch 995  Loss 0.7364621  Acc 0.68  \n",
      "Epoch 996  Loss 0.73617226  Acc 0.68  \n",
      "Epoch 997  Loss 0.7358752  Acc 0.68  \n",
      "Epoch 998  Loss 0.73558515  Acc 0.68  \n",
      "Epoch 999  Loss 0.7353  Acc 0.68  \n",
      "Epoch 1000  Loss 0.7350047  Acc 0.68  \n",
      "Epoch 1001  Loss 0.7347104  Acc 0.68  \n",
      "Epoch 1002  Loss 0.73441607  Acc 0.68  \n",
      "Epoch 1003  Loss 0.7341247  Acc 0.68  \n",
      "Epoch 1004  Loss 0.73383355  Acc 0.68  \n",
      "Epoch 1005  Loss 0.7335443  Acc 0.68  \n",
      "Epoch 1006  Loss 0.7332415  Acc 0.68  \n",
      "Epoch 1007  Loss 0.732947  Acc 0.68  \n",
      "Epoch 1008  Loss 0.73265135  Acc 0.68  \n",
      "Epoch 1009  Loss 0.7323598  Acc 0.68  \n",
      "Epoch 1010  Loss 0.7320646  Acc 0.68  \n",
      "Epoch 1011  Loss 0.7317739  Acc 0.68  \n",
      "Epoch 1012  Loss 0.7314857  Acc 0.68  \n",
      "Epoch 1013  Loss 0.7311924  Acc 0.68  \n",
      "Epoch 1014  Loss 0.7309022  Acc 0.68  \n",
      "Epoch 1015  Loss 0.7306084  Acc 0.68  \n",
      "Epoch 1016  Loss 0.7303176  Acc 0.68  \n",
      "Epoch 1017  Loss 0.7300235  Acc 0.68  \n",
      "Epoch 1018  Loss 0.72972745  Acc 0.68  \n",
      "Epoch 1019  Loss 0.7294308  Acc 0.68  \n",
      "Epoch 1020  Loss 0.7291473  Acc 0.68  \n",
      "Epoch 1021  Loss 0.728855  Acc 0.68  \n",
      "Epoch 1022  Loss 0.72856575  Acc 0.68  \n",
      "Epoch 1023  Loss 0.7282769  Acc 0.68  \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NB_EPOCHS):\n",
    "    print('Epoch', epoch, ' ', end='')\n",
    "    \n",
    "    data_x = x_train\n",
    "    data_y = y_train\n",
    "    \n",
    "    r1, r2, r3, r4 = sess.run([train, loss, probs, out], feed_dict={x: data_x, y: data_y})\n",
    "    print('Loss', r2, ' ', end='')\n",
    "    acc = 0\n",
    "    tot = 0\n",
    "    for i, j in enumerate(np.argmax(r3, axis=1)):\n",
    "        vec = np.zeros(class_count)\n",
    "        vec[j] = 1.0\n",
    "        if np.allclose(data_y[i], vec):\n",
    "            acc += 1\n",
    "        tot += 1\n",
    "    print('Acc', acc/tot, ' ')\n",
    "    \n",
    "    pos = r4\n",
    "    col = np.argmax(data_y, axis=1)\n",
    "    fig.clf()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.elev = 15.0\n",
    "    ax.azim = 15.0\n",
    "    ax.scatter(pos[:, 0], pos[:, 1], pos[:, 2], c=col, cmap='Set1')\n",
    "    fig.savefig(f'frames/{epoch}.png')\n",
    "    \n",
    "    while data_x.any():\n",
    "        bs = BATCH_SIZE\n",
    "        if data_x.shape[0] < BATCH_SIZE:\n",
    "            bs = data_x.shape[0]\n",
    "        \n",
    "        indices = np.arange(0, data_x.shape[0], step=1)\n",
    "        batch_indices = np.random.choice(indices, size=bs, replace=False)\n",
    "\n",
    "        batch_x = data_x[batch_indices]\n",
    "        batch_y = data_y[batch_indices]\n",
    "            \n",
    "        sess.run(train, feed_dict={x: batch_x, y: batch_y})\n",
    "        \n",
    "        data_x = np.delete(data_x, batch_indices, axis=0)\n",
    "        data_y = np.delete(data_y, batch_indices, axis=0)\n",
    "        indices = np.delete(indices, batch_indices, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C:\\Users\\August\\Downloads\\ffmpeg.exe -i %d.png out.webm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"./frames/out.webm\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Video('./frames/out.webm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "      <th>class</th>\n",
       "      <th>index</th>\n",
       "      <th>onehot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length  sepal width  petal length  petal width        class  index  \\\n",
       "8           4.4          2.9           1.4          0.2  Iris-setosa      0   \n",
       "\n",
       "            onehot  \n",
       "8  [1.0, 0.0, 0.0]  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_example = data.sample(1)\n",
    "random_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length  sepal width  petal length  petal width\n",
       "8           4.4          2.9           1.4          0.2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_x = random_example[['sepal length', 'sepal width', 'petal length', 'petal width']]\n",
    "example_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8    [1.0, 0.0, 0.0]\n",
       "Name: onehot, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_y = random_example['onehot']\n",
    "example_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5216935 , 0.30448022, 0.17382617]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = sess.run(probs, feed_dict={x: example_x })\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iris-setosa'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_class(p.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
